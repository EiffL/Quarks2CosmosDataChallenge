{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/Quarks2CosmosDataChallenge/blob/colab/notebooks/PartI-DifferentiableForwardModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDcRQNGAhv4r"
      },
      "source": [
        "# Guided Data Challenge Part I: Introduction to Differentiable Programming in Jax\n",
        "\n",
        "Author:\n",
        " - [@EiffL](https://github.com/EiffL) (Fancois Lanusse)\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "The overall aim of this challenge is to solve image inverse problem (denoising, deconvolution, inpainting) in a fully Bayesian way i.e. drawing from a posterior distribution, and using a deep generative model as a prior.\n",
        "\n",
        "More concretely, we will consider a deconvolution problem where one aims to recover a high quality image from a low resolution image degraded by an instrumental response. The following figure provides an illustration of the problem:\n",
        "\n",
        "![](https://www.researchgate.net/profile/Alan-Heavens/publication/1910339/figure/fig1/AS:394636699947010@1471100129570/Illustration-of-the-forward-problem-The-upper-panels-show-how-the-original-galaxy-image.png)\n",
        "\n",
        "What we will have access to is an observed galaxy on the right of the chain, and we will try to recover the high resolution image before PSF convolution.\n",
        "\n",
        "\n",
        "We will use two datasets as the basis for this challenge:\n",
        "- Galaxies from the Hubble Space Telescope COSMOS survey, described for instance [here](https://arxiv.org/pdf/1308.4982.pdf#page=24) under `REAL GALAXY DATASET`.\n",
        "- Galaxies of the [HSC Survey](https://hsc-release.mtk.nao.ac.jp/doc/), taken with the ground-based Subaru telescope\n",
        "\n",
        "This guided data challenge will be split into 3 parts. In this first part, we will be discovering the tools and datasets.\n",
        "\n",
        "\n",
        "### Learning objectives\n",
        "\n",
        "The goal of this first part of this guided data challenge is to get familiar with the datasets, and more importantly with the tools needed for the next steps. You will put into practice the following:\n",
        "\n",
        "  - Build a forward model using Jax\n",
        "  - Build a likelihood using TensorFlow Probability\n",
        "  - Retrieve maximum likelihood solution by optimization\n",
        "  - (Stretch goal): Learn to write a Neural Network with DeepMind's Haiku\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies and accessing data"
      ],
      "metadata": {
        "id": "0ktLeF7bjnbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/EiffL/Quarks2CosmosDataChallenge.git\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!mkdir galsim\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ],
      "metadata": {
        "id": "p_56Uqv6h0QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticating and mounting cloud data storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcsfuse --implicit-dirs galsim galsim"
      ],
      "metadata": {
        "id": "rypc1fA8iK9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrcxfNpEhv4s"
      },
      "source": [
        "## Before we begin... A few words about JAX\n",
        "\n",
        "What's the best Deep Learning framework? TensorFlow? PyTorch? Wrong: all you ever want is JAX :-)\n",
        "\n",
        "![](https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png)\n",
        "\n",
        "JAX = NumPy + autograd + XLA\n",
        "\n",
        "In other words, it looks like pure NumPy, but is automatically differentiable, and runs on XLA (i.e. GPU accelerated). Checkout the [full documentation](https://jax.readthedocs.io/en/latest/index.html) to discover the many awesome features JAX has to offer.\n",
        "\n",
        "#### Autodiff\n",
        "\n",
        "For our purposes, the most interesting feature of JAX will be automatic differentiation, which is performed using `jax.grad`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ALu_s-Jhv4s"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb5_06BRhv4t"
      },
      "outputs": [],
      "source": [
        "# Let's build a function of x that returns a scalar\n",
        "def f(x):\n",
        "    y = 2*x+1\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvRuEZu3hv4u"
      },
      "outputs": [],
      "source": [
        "# To get the derivative of this function, I simply use jax.grad\n",
        "df_dx = jax.grad(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p63iXDahv4u"
      },
      "outputs": [],
      "source": [
        "df_dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oN9MfsVhv4u"
      },
      "source": [
        "`jax.grad` has *transformed* my function `f` into a new function that comptutes its gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntSK2Oefhv4u"
      },
      "outputs": [],
      "source": [
        "u = jnp.linspace(0,1)\n",
        "plot(u, f(u), label='f')\n",
        "plot(u,jax.vmap(df_dx)(u), label='df/dx')\n",
        "legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kqbIyaPhv4u"
      },
      "source": [
        "#### Vectorization with `vmap`\n",
        "\n",
        "Another awesome feature of JAX is the ability to batch any function using the `jax.vmap` transformation. Say you have a function that works on a single example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVJ9OkAxhv4v"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    # Expect x to be of shape [16, 16]\n",
        "    nx, ny = x.shape\n",
        "    x = x + jnp.ones([nx,ny])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIP0aKdbhv4v"
      },
      "outputs": [],
      "source": [
        "x = jnp.zeros([16,16])\n",
        "f(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjNNW1wHhv4v"
      },
      "outputs": [],
      "source": [
        "# But this function wouldn't work if I had a batch of images\n",
        "x = jnp.zeros([1,16,16])\n",
        "f(x).shape # THIS SHOULD FAIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjWxImKyhv4v"
      },
      "outputs": [],
      "source": [
        "# But I can use jax magic\n",
        "f_batched = jax.vmap(f)\n",
        "\n",
        "x = jnp.zeros([1,16,16])\n",
        "f_batched(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBuPi28Phv4v"
      },
      "source": [
        "Tada!\n",
        "\n",
        "#### JIT compilation\n",
        "\n",
        "The final super useful thing to know about JAX is that you can JIT (Just In Time) compile any function, they will be compiled as an XLA graph and completely bypass Python to run directly under XLA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOKnoiKGhv4v"
      },
      "outputs": [],
      "source": [
        "def f(x, A):\n",
        "    return A.dot(x)\n",
        "\n",
        "f_jitted = jax.jit(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCM3rZqAhv4v"
      },
      "outputs": [],
      "source": [
        "x = randn(128)\n",
        "A = randn(128,128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxFjYbXYhv4v"
      },
      "outputs": [],
      "source": [
        "f_jitted(x,A);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je65LTIwhv4w"
      },
      "source": [
        "On this example, jitting won't make much of a difference because it is  a tiny function, but we will see later that we can JIT the computation of the loss and update of a full neural network, which then becomes lightning fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmK-Gl-thv4w"
      },
      "source": [
        "## Step I: Loading the data\n",
        "\n",
        "In this section, we load the datasets for our space-based and ground-based images. Both datasets are conviently accessible as [TensorFlow Datasets](https://www.tensorflow.org/datasets) datasets, you simply need the following imports to access them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idebrU00hv4w"
      },
      "outputs": [],
      "source": [
        "import quarks2cosmos.datasets\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvtB4FChv4w"
      },
      "source": [
        "#### COSMOS Dataset\n",
        "\n",
        "The COSMOS dataset is built from the GalSim COSMOS sample (see [here](https://galsim-developers.github.io/GalSim/_build/html/real_gal.html) for details), it contains postage stamps of real galaxies,  **convolved with an isotropized HST PSF** which is the same for all galaxies. These images are thus completely standardized, and the PSF doesn't vary from one object to the other.\n",
        "\n",
        "Properties of COSMOS stamps:\n",
        "- TRAIN and TEST splits with 40000 and 10000 images respectively\n",
        "- Constant isotropic PSF\n",
        "- Pixel scale 0.03\n",
        "- Postage stamp size: 101x101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFAVe3HMhv4w"
      },
      "outputs": [],
      "source": [
        "dset_cosmos = tfds.load(\"Cosmos/23.5\", split=tfds.Split.TRAIN,\n",
        "                        data_dir='galsim/tensorflow_datasets') # Load the TRAIN split\n",
        "dset_cosmos = dset_cosmos.as_numpy_iterator()                  # Convert the dataset to numpy iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIUdGZb6hv4w"
      },
      "outputs": [],
      "source": [
        "# Extract a new example from the dataset\n",
        "cosmos = next(dset_cosmos)\n",
        "\n",
        "figure(figsize=[10,5])\n",
        "subplot(121)\n",
        "imshow(cosmos['image'],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(122)\n",
        "imshow(cosmos['psf'],cmap='gray')\n",
        "title('PSF');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lgrSw75hv4w"
      },
      "source": [
        "#### HSC Dataset\n",
        "\n",
        "The HSC galaxies are drawn from the PDR2 data release and extracted using the excellent [unagi](https://github.com/dr-guangtou/unagi) tool. Details of how these objects are selected can be found in this accompanying [notebook](HSCDataPreparation.ipynb), but essentially these are i-band stamps of galaxies with imag $\\in [21, 23.5]$, also verifying the `forced.i_pixelflags_interpolatedcenter` condition, meaning they contain a number of interpolated pixels which intersect the center of the object. The stamps are at resolution 0.168 arcsec, and the PSF is provided for each galaxy.\n",
        "\n",
        "Properties of HSC stamps:\n",
        "- Single TRAIN split of ~10000 examples\n",
        "- PSF, pixel masks, and variance map provided for every image\n",
        "- Pixel scale: 0.168\n",
        "- Postage stamp size: 41x41"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCxLlFhqhv4w"
      },
      "outputs": [],
      "source": [
        "dset_hsc = tfds.load(\"HSC\", split=tfds.Split.TRAIN,\n",
        "                     data_dir='galsim/tensorflow_datasets')\n",
        "dset_hsc = dset_hsc.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXOwun5Dhv4w"
      },
      "outputs": [],
      "source": [
        "# Extract a new example from the dataset\n",
        "hsc = next(dset_hsc)\n",
        "\n",
        "figure(figsize=[20,5])\n",
        "subplot(141)\n",
        "imshow(hsc['image'],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(142)\n",
        "imshow(hsc['psf'],cmap='gray')\n",
        "title('PSF')\n",
        "subplot(143)\n",
        "imshow(hsc['mask'] == 44,cmap='gray')\n",
        "title('Interpolated pixels')\n",
        "subplot(144)\n",
        "imshow(hsc['variance'],cmap='gray')\n",
        "title('Variance plane');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAe8CxGOhv4w"
      },
      "source": [
        "## Step II: Building a forward model\n",
        "\n",
        "\n",
        "\n",
        "We only provides the fundamentals here, and we encourage the interested reader to directly reach out to challenge organizers to learn more :-)\n",
        "\n",
        "The problems we will be addressing in this challenge can be written as the following:\n",
        "\n",
        "$$y = \\Pi \\ast \\left(  \\mathbf{P} \\ (\\Pi_{HST}^{-1} \\ast x ) \\right) + n \\qquad \\mbox{with} \\qquad n \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
        "where $y$ are the measurements and where:\n",
        "- $x$ would be a space-based HST galaxy image we would like to recover\n",
        "- $\\Pi_{HST}$ is the HST PSF\n",
        "- $\\mathbf{P}$ is a resampling operator to go from the pixel scale of HST to that of HSC images\n",
        "- $\\Pi$ is a given HSC PSF\n",
        "- $\\sigma$ is the noise variance of HSC observations.  \n",
        "\n",
        "Typically, one would use the [GalSim software](https://github.com/GalSim-developers/GalSim) to perform these manipulations (convolution/deconvolution/resampling) but GalSim is not differentiable :-(\n",
        "\n",
        "For the purpose of this data challenge we provide JAX equivalent functions as part of the `quarks2cosmos.galjax` and this will allow us to write a differentiable forward model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7V6ur1thv4w"
      },
      "outputs": [],
      "source": [
        "from quarks2cosmos import galjax as gj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QKRubUPhv4w"
      },
      "outputs": [],
      "source": [
        "def simulate_hsc(x, in_psf, out_psf):\n",
        "    \"\"\" This function will simulate an image at HSC resolution given an image at HST resolution,\n",
        "    accounting for input PSF and convolving by output PSF\n",
        "    Args:\n",
        "        x: HST resolution image (MUST BE ODD SIZE!!!!)\n",
        "        in_psf: HST PSF\n",
        "        out_psf: HSC PSF\n",
        "    Returns:\n",
        "        y: HSC simulated image of size [41,41]\n",
        "    \"\"\"\n",
        "    y = gj.deconvolve(x, in_psf)         # Deconvolve by input PSF\n",
        "    y = gj.kresample(y, 0.03, 0.168, 41) # Resample image to HSC grid\n",
        "    y = gj.convolve(y,  out_psf)         # Reconvolve by HSC PSF\n",
        "    return 2.587*y                       # Conversion factor for the flux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6s0itw8hv4w"
      },
      "outputs": [],
      "source": [
        "# Let's apply it to our input images\n",
        "im = simulate_hsc(cosmos['image'], cosmos['psf'], hsc['psf'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyonMmTShv4w"
      },
      "outputs": [],
      "source": [
        "subplot(121)\n",
        "imshow(cosmos['image'])\n",
        "title('As seen from Hubble')\n",
        "subplot(122)\n",
        "imshow(im)\n",
        "title('As seen from Subaru');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1zMK6bohv4x"
      },
      "source": [
        "So far our model only generates an HSC image without noise, but we are provided with a noise variance map for each observation, so we can actually write down a likelihood function.\n",
        "\n",
        "#### Data likelihood using TensorFlow Probability\n",
        "\n",
        "This is an opportunity to introduce the awesome [TensorFlow Probabilty](https://www.tensorflow.org/probability) package, which also works with JAX ;-) with the following imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iftqI6Vehv4x"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXXp4l00hv4x"
      },
      "source": [
        "As mentioned in the model above, we assume a Gaussian likelihood for the observations:\n",
        "\n",
        "$$ p(y | x) = \\mathcal{N}(y ; f(x), \\Sigma) $$\n",
        "\n",
        "where $f$ is our forward model defined above, and $\\Sigma$ is our noise covariance, which we assume diagonal here. In TFP, we can create this likelihood very easily as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kVtKfcdhv4x"
      },
      "outputs": [],
      "source": [
        "likelihood = tfd.Independent(tfd.Normal(loc=simulate_hsc(cosmos['image'], cosmos['psf'], hsc['psf']),\n",
        "                                        scale=jnp.sqrt(hsc['variance'])),\n",
        "                             reinterpreted_batch_ndims=2) # This is to make sure TFP understand we have a 2d image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF7v0EdMhv4x"
      },
      "outputs": [],
      "source": [
        "likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF6TRugChv4x"
      },
      "source": [
        "Once you have this likelihood, you can for instance sample from it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Jf6IELhv4x"
      },
      "outputs": [],
      "source": [
        "im_noise = likelihood.sample(seed=jax.random.PRNGKey(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_l05caNhv4x"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[15,5])\n",
        "subplot(131)\n",
        "imshow(cosmos['image'])\n",
        "title('As seen from Hubble')\n",
        "subplot(132)\n",
        "imshow(im)\n",
        "title('As seen from Subaru')\n",
        "subplot(133)\n",
        "imshow(im_noise)\n",
        "title('As seen from Subaru + Noise')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyKTOFeShv4y"
      },
      "source": [
        "Or evaluate the likelihood at any given point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn9Pi6ehhv4y"
      },
      "outputs": [],
      "source": [
        "likelihood.log_prob(im_noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dggm02Y3hv4y"
      },
      "source": [
        "## Step III: Solving an inverse problem by optimization\n",
        "\n",
        "Now that we have the ability to write a forward model and evaluate a likelihood, we can get started on trying to solve our inverse problem.\n",
        "\n",
        "We will start by trying to recover a maximum likelihood solution\n",
        "$$ \\hat{x} = \\arg \\max_{x} \\log p(y | x) $$\n",
        "\n",
        "Let's simulate some observations that will act as our observables $y$ and for which we will know the truth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpDjCJpwhv4y"
      },
      "outputs": [],
      "source": [
        "# We will use this galaxy as are reference here\n",
        "x_true = cosmos['image']\n",
        "y_obs = im_noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60c8fAjshv4y"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[10,5])\n",
        "subplot(121)\n",
        "imshow(x_true)\n",
        "title('Hubble image to recover')\n",
        "subplot(122)\n",
        "imshow(y_obs)\n",
        "title('Observed image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk6K7DRLhv4y"
      },
      "source": [
        "#### Writing a log likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDk0-vzDhv4y"
      },
      "outputs": [],
      "source": [
        "def log_prob(x):\n",
        "    \"\"\" Returns the value of the log likelihood of the observed data for a give x\n",
        "    \"\"\"\n",
        "    likelihood = tfd.Independent(tfd.Normal(loc=simulate_hsc(x, cosmos['psf'], hsc['psf']),\n",
        "                                            scale=jnp.sqrt(hsc['variance'])),\n",
        "                             reinterpreted_batch_ndims=2)\n",
        "    return likelihood.log_prob(y_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_dKx4wOhv4y"
      },
      "source": [
        "#### Creating an optimizer\n",
        "Once we have a likelihood, we now want to optimize it, for this we will use the optax library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3ZVG3vGhv4y"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "optimizer = optax.adam(0.01) # Instantiate an ADAM optimizer\n",
        "\n",
        "# Create a variable to store the solution\n",
        "x = jnp.zeros([101, 101])\n",
        "\n",
        "# Initialize the optimizer\n",
        "opt_state = optimizer.init(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFdN46oKhv4y"
      },
      "source": [
        "#### Writing an update function\n",
        "\n",
        "The way optax and most JAX neural network libraries work is that the optimizer provides a function that computes how to update the parameters given the gradients of the loss function. One iteration of the optimization will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uIMq3Gchv4y"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def update(x, opt_state):\n",
        "    \"\"\" Computes update to parameters x\n",
        "    \"\"\"\n",
        "    logp, grads = jax.value_and_grad(log_prob)(x)              # Takes the gradients of the likelihood\n",
        "    updates, opt_state = optimizer.update(-grads, opt_state)   # Computes ADAM update to maximize likelihood\n",
        "    x = optax.apply_updates(x, updates)                        # Apply update to parameters\n",
        "    return logp, x, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y1qZdQihv4y"
      },
      "source": [
        "#### Running the optimization loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JV8fQgchv4z"
      },
      "outputs": [],
      "source": [
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgRGDRc4hv4z"
      },
      "outputs": [],
      "source": [
        "for i in range(200):\n",
        "    logp, x, opt_state = update(x, opt_state)\n",
        "    losses.append(logp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CosdkZ_Shv4z"
      },
      "outputs": [],
      "source": [
        "loglog(abs(jnp.array(losses)))\n",
        "xlabel('step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGp7nDsZhv4z"
      },
      "outputs": [],
      "source": [
        "# Let's look at the solution!\n",
        "figure(figsize=[15,5])\n",
        "subplot(141)\n",
        "imshow(x)\n",
        "title('Current Solution')\n",
        "subplot(142)\n",
        "imshow(simulate_hsc(x, cosmos['psf'], hsc['psf']))\n",
        "title('As seen from Subaru')\n",
        "subplot(143)\n",
        "imshow(y_obs)\n",
        "title('Observations')\n",
        "subplot(144)\n",
        "imshow(y_obs - simulate_hsc(x, cosmos['psf'], hsc['psf']))\n",
        "title('Residuals');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpXmqaYshv4z"
      },
      "source": [
        "And there you have it, we have found **a** solution, which fits the observations very well! Now, is it a good solution? Given that the inverse problem is very ill-posed and that we have no prior, we get a solution that doesn't appear very physical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsz1zlUphv4z"
      },
      "source": [
        "###  Adding the mask and proximal regularisation\n",
        "\n",
        "To make things a bit more difficult, let's now consider a situation where we don't observe part of the galaxy, due to cosmic rays. Our data can be written like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9xRmzZjhv4z"
      },
      "outputs": [],
      "source": [
        "x_true = cosmos['image']\n",
        "cr_mask = 1.*(hsc['mask'] == 44)\n",
        "y_obs = im_noise * (1 - cr_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeLKnpnIhv4z"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[15,5])\n",
        "subplot(131)\n",
        "imshow(x_true)\n",
        "title('Hubble image to recover')\n",
        "subplot(132)\n",
        "imshow(y_obs)\n",
        "title('Observed image')\n",
        "subplot(133)\n",
        "imshow(cr_mask)\n",
        "title('Cosmic Ray mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPv_jIryhv4z"
      },
      "source": [
        "We can modify the likelihood to account for this mask, for instance by saying the regions in the mask have very high variance. This means that the value these pixels are not to be trusted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsNLSjQJhv4z"
      },
      "outputs": [],
      "source": [
        "def log_prob(x):\n",
        "    likelihood = tfd.Independent(tfd.Normal(simulate_hsc(x, cosmos['psf'], hsc['psf']),\n",
        "                                            jnp.sqrt(hsc['variance'] + cr_mask*1e3)), # We add a high noise component in mask\n",
        "                             reinterpreted_batch_ndims=2)\n",
        "    return likelihood.log_prob(y_obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUpK10Yqhv4z"
      },
      "outputs": [],
      "source": [
        "optimizer = optax.adam(0.002)\n",
        "\n",
        "# Create a parameter\n",
        "x = jnp.zeros([101, 101])\n",
        "\n",
        "opt_state = optimizer.init(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsdLVkUmhv4z"
      },
      "source": [
        "For fun we will add a proximal constraint in the optimization, to constrain the solution to be positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryX2ZUk4hv4z"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def update(x, opt_state):\n",
        "    logp, grads = jax.value_and_grad(lambda x: log_prob(x))(x)\n",
        "    updates, opt_state = optimizer.update(-grads, opt_state)\n",
        "\n",
        "    # Apply gradient descent\n",
        "    x = optax.apply_updates(x, updates)\n",
        "\n",
        "    # Apply proximal constraints\n",
        "    x = jnp.abs(x)\n",
        "\n",
        "    return logp, x, opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrj8ozCjhv4z"
      },
      "outputs": [],
      "source": [
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN04TggLhv4z"
      },
      "outputs": [],
      "source": [
        "for i in range(1000):\n",
        "    logp, x, opt_state = update(x, opt_state)\n",
        "    losses.append(logp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdybjzHShv40"
      },
      "outputs": [],
      "source": [
        "loglog(abs(np.array(losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0IZ6dlahv40"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[15,5])\n",
        "subplot(141)\n",
        "imshow(x)\n",
        "title('Solution')\n",
        "subplot(142)\n",
        "imshow(simulate_hsc(x, cosmos['psf'], hsc['psf']))\n",
        "title('As seen from Subaru')\n",
        "subplot(143)\n",
        "imshow(y_obs)\n",
        "title('Observations')\n",
        "subplot(144)\n",
        "imshow((im_noise - simulate_hsc(x, cosmos['psf'], hsc['psf'])))\n",
        "title('Residuals');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzH1TfMQhv40"
      },
      "source": [
        "## Step IV: Naive Deep Learning  Approach\n",
        "\n",
        "Now that we have covered the basics of how to manipulate the problem, we can try a first Deep Learning approach. Can a CNN recover the space-based image?\n",
        "\n",
        "It should be noted that this is really not the approach we will try to follow in this data challenge, but this is a good opporrtunity to learn how to write and train a neural network in JAX.\n",
        "\n",
        "We will use the [Haiku](https://github.com/deepmind/dm-haiku) library from DeepMind for all neural network related things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jOvG7bIhv40"
      },
      "source": [
        "#### Building data pipeline\n",
        "\n",
        "Let's start with a pipeline creating examples of HST and HSC images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPw9Avjphv40"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def preprocess(example):\n",
        "    \"\"\" Augment COSMOS dataset with random flips\n",
        "    \"\"\"\n",
        "    x = example['image'][...,tf.newaxis]\n",
        "    x = tf.image.flip_left_right(x)\n",
        "    x = tf.image.flip_up_down(x)\n",
        "    return {'image':x[...,0], 'psf':example['psf']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRj_F88fhv40"
      },
      "outputs": [],
      "source": [
        "# Load COSMOS\n",
        "dset_cosmos = tfds.load(\"Cosmos/23.5\", split=tfds.Split.TRAIN,\n",
        "                        data_dir='galsim/tensorflow_datasets')\n",
        "dset_cosmos = dset_cosmos.cache()\n",
        "dset_cosmos = dset_cosmos.repeat()\n",
        "dset_cosmos = dset_cosmos.map(preprocess)\n",
        "dset_cosmos = dset_cosmos.shuffle(40000)\n",
        "\n",
        "# Load HSC\n",
        "dset_hsc = tfds.load(\"HSC\", split=tfds.Split.TRAIN,\n",
        "                        data_dir='galsim/tensorflow_datasets')\n",
        "dset_hsc = dset_hsc.cache()\n",
        "dset_hsc = dset_hsc.repeat()\n",
        "dset_hsc = dset_hsc.shuffle(10000)\n",
        "\n",
        "# Combine both datasets\n",
        "combined_dset = tf.data.Dataset.zip((dset_cosmos, dset_hsc))\n",
        "combined_dset = combined_dset.batch(64)                      # Adds batching\n",
        "combined_dset = combined_dset.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg0MboRJhv40"
      },
      "outputs": [],
      "source": [
        "batch = next(combined_dset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7YM4wVYhv40"
      },
      "outputs": [],
      "source": [
        "# A batch contains examples from both datasets\n",
        "cosmos, hsc = batch\n",
        "\n",
        "figure(figsize=[10,5])\n",
        "subplot(121)\n",
        "imshow(cosmos['image'][0],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(122)\n",
        "imshow(cosmos['psf'][0],cmap='gray')\n",
        "title('PSF');\n",
        "\n",
        "figure(figsize=[20,5])\n",
        "subplot(141)\n",
        "imshow(hsc['image'][0],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(142)\n",
        "imshow(hsc['psf'][0],cmap='gray')\n",
        "title('PSF')\n",
        "subplot(143)\n",
        "imshow(hsc['mask'][0] == 44,cmap='gray')\n",
        "title('Interpolated pixels')\n",
        "subplot(144)\n",
        "imshow(hsc['variance'][0],cmap='gray')\n",
        "title('Variance plane');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIpNcEBzhv40"
      },
      "source": [
        "#### Defining a Neural Network\n",
        "\n",
        "For this kind of thing, one may think some kind of Unet might be able to solve this deconvolution task.\n",
        "\n",
        "In Haiku, Neural Networks are defined by creating a `hk.Module` subclass and populating its `__call__` member"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm42qnuJhv40"
      },
      "outputs": [],
      "source": [
        "import haiku as hk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUHnVzEzhv40"
      },
      "outputs": [],
      "source": [
        "class Unet(hk.Module):\n",
        "    \"\"\" The most trivial Unet possible\n",
        "    \"\"\"\n",
        "    def __call__(self, x):\n",
        "        x = hk.Conv2D(16, kernel_shape=3)(x)\n",
        "        l1 = jax.nn.leaky_relu(x)\n",
        "        x = hk.avg_pool(l1, window_shape=3, strides=2, padding='SAME')\n",
        "\n",
        "        x = hk.Conv2D(32, kernel_shape=3)(x)\n",
        "        l2 = jax.nn.leaky_relu(x)\n",
        "        x = hk.avg_pool(l2,  window_shape=3, strides=2, padding='SAME')\n",
        "\n",
        "        x = hk.Conv2D(64, kernel_shape=3)(x)\n",
        "        l3 = jax.nn.leaky_relu(x)\n",
        "        x = hk.avg_pool(l3, window_shape=3, strides=2, padding='SAME')\n",
        "\n",
        "        x = hk.Conv2D(128, kernel_shape=3)(x)\n",
        "        x = jax.nn.leaky_relu(x)\n",
        "\n",
        "        x = hk.Conv2DTranspose(64, kernel_shape=3, stride=2)(x)\n",
        "        x = jnp.concatenate([x, l3], axis=-1)\n",
        "\n",
        "        x = jax.nn.leaky_relu(x)\n",
        "        x = hk.Conv2DTranspose(32, kernel_shape=3, stride=2)(x)\n",
        "        x = jnp.concatenate([x, l2], axis=-1)\n",
        "\n",
        "        x = jax.nn.leaky_relu(x)\n",
        "        x = hk.Conv2DTranspose(16, kernel_shape=5, stride=2)(x)\n",
        "        x = jnp.concatenate([x, l1], axis=-1)\n",
        "\n",
        "        x = hk.Conv2D(1, kernel_shape=5)(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPKuTKehhv40"
      },
      "outputs": [],
      "source": [
        "# Transform model into pure functions\n",
        "model = hk.without_apply_rng(hk.transform(lambda x: Unet()(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LE_uU1khv40"
      },
      "outputs": [],
      "source": [
        "# Initialize model parameters\n",
        "params = model.init(jax.random.PRNGKey(0), jnp.zeros([1,128,128,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdR2oXOshv41"
      },
      "outputs": [],
      "source": [
        "def loss_fn(params, rng, batch, return_images=False):\n",
        "    # Create mock observations\n",
        "    cosmos, hsc = batch\n",
        "\n",
        "    def prep_data(cosmos, hsc):\n",
        "        \"\"\" Prepares data for the neural network\n",
        "        \"\"\"\n",
        "        obs = tfd.Independent(tfd.Normal(jax.vmap(simulate_hsc)(cosmos['image'], cosmos['psf'], hsc['psf']),\n",
        "                         jnp.sqrt(hsc['variance'])),\n",
        "                             reinterpreted_batch_ndims=2).sample(seed=rng)\n",
        "\n",
        "        # Rescale obs to same pixel scale to make the life of the network easier\n",
        "        x = jax.image.scale_and_translate(obs, [len(obs), 128, 128], [1,2],\n",
        "                                            jnp.array([5.6, 5.6]),\n",
        "                                            jnp.array([128/2 - 41/2*5.6 ,128/2 - 41/2*5.6 ]),\n",
        "                                            jax.image.ResizeMethod.CUBIC)\n",
        "\n",
        "        # Resize input and outputs for the network\n",
        "        im  = jnp.pad(cosmos['image'], [[0,0], [14,13] , [14,13]])\n",
        "\n",
        "        # Adding channel dimension\n",
        "        im = jnp.expand_dims(im, -1)\n",
        "        x = jnp.expand_dims(x, -1)\n",
        "        return im, x, obs\n",
        "\n",
        "    im, x, obs = prep_data(cosmos, hsc )\n",
        "\n",
        "    # Apply neural network\n",
        "    rec = model.apply(params, x)\n",
        "\n",
        "    loss = jnp.mean( jnp.sum( (rec - im)**2, axis=[1, 2, 3]))\n",
        "\n",
        "    if return_images:\n",
        "        return im, obs, x, rec, loss\n",
        "    else:\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS2tc3ZOhv41"
      },
      "outputs": [],
      "source": [
        "optimizer = optax.adam(0.001)\n",
        "opt_state = optimizer.init(params)\n",
        "rng_seq = hk.PRNGSequence(12)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NBYKeb6hv41"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def update(params, rng, opt_state):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, rng, batch)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    # Apply gradient descent\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return loss, params, opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj2kQQLghv41"
      },
      "outputs": [],
      "source": [
        "for i in range(10000):\n",
        "    batch = next(combined_dset)\n",
        "    loss, params, opt_state = update(params, next(rng_seq), opt_state)\n",
        "    losses.append(loss)\n",
        "    if i %100 ==0:\n",
        "        print('step',i,loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP4ndaaJhv41"
      },
      "outputs": [],
      "source": [
        "loglog(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxuXh9nthv41"
      },
      "outputs": [],
      "source": [
        "# Create a test dataset\n",
        "dset_cosmos_test = tfds.load(\"Cosmos/23.5\",\n",
        "                        split=tfds.Split.TEST)\n",
        "dset_cosmos_test = dset_cosmos_test.cache()\n",
        "dset_cosmos_test = dset_cosmos_test.repeat()\n",
        "\n",
        "dset_hsc = tfds.load(\"HSC\",\n",
        "                     split=tfds.Split.TRAIN)\n",
        "dset_hsc = dset_hsc.cache()\n",
        "dset_hsc = dset_hsc.repeat()\n",
        "dset_hsc = dset_hsc.shuffle(10000)\n",
        "\n",
        "combined_dset_test = dset_hsc.zip((dset_cosmos_test , dset_hsc))\n",
        "combined_dset_test = combined_dset_test.batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRNlZ75Ghv41"
      },
      "outputs": [],
      "source": [
        "it_test  = combined_dset_test.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE8LFSWzhv41"
      },
      "outputs": [],
      "source": [
        "batch = next(it_test)\n",
        "im, obs, y, rec, loss = loss_fn(params, next(rng_seq), batch, return_images=True)\n",
        "cosmos, hsc = batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "8QUvQNMshv41"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    figure(figsize=[20,5])\n",
        "    subplot(141)\n",
        "    imshow(im[i])\n",
        "    subplot(142)\n",
        "    imshow(obs[i])\n",
        "    subplot(143)\n",
        "    imshow(rec[i])\n",
        "    subplot(144)\n",
        "    # Removing the padding\n",
        "    x = rec[i][14:-13:,14:-13:,0]\n",
        "    x = simulate_hsc(x, cosmos['psf'][i], hsc['psf'][i])\n",
        "    # Show residuals on observations\n",
        "    imshow(obs[i] - x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkI5QiBNhv41"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}