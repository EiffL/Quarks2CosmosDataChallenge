{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/Quarks2CosmosDataChallenge/blob/colab/notebooks/MappingDarkMatterDataChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhfOcMaUvT7s"
      },
      "source": [
        "# Quarks to Cosmos with AI Data Challenge: Mapping the Distribution of Dark Matter\n",
        "\n",
        "\n",
        "Authors:\n",
        " - [@EiffL](https://github.com/EiffL) (Fancois Lanusse)\n",
        " - [@b-remy](https://github.com/b-remy) (Benjamin Remy)\n",
        "\n",
        "## Overview\n",
        "\n",
        "The aim of this data challenge is to reconstruct a map of the Dark Matter distribution in a region of the sky, through the weak gravitational lensing effect. What makes this problem challenging is that we want to not only recover an estimate of this map, but also infer a full posterior distribution.\n",
        "\n",
        "As our data, we will use the weak lensing shape catalog from the [HSC survey](https://hsc-release.mtk.nao.ac.jp/doc/index.php/s16a-shape-catalog-pdr2/), as described in [arXiv:1705.06745](https://arxiv.org/abs/1705.06745) led by CMU's Prof. Mandelbaum.\n",
        "\n",
        "We will also use simulation data to provide a data driven prior, for the purpose of this challenge we will use the [MassiveNuS](http://columbialensing.org/) suite of weak lensing simulations described in [arXiv:1711.10524](https://arxiv.org/abs/1711.10524) and developed by Dr. Jia Liu.\n",
        "\n",
        "Our goal will be to produce a map and uncertainty estimates for a portion of the XMM field which you can navigate [here](https://hsc-release.mtk.nao.ac.jp/hscMap-pdr2/app/#/?_=%7B%22view%22%3A%7B%22a%22%3A0.6457718473821972,%22d%22%3A-0.0785398191711961,%22fovy%22%3A0.014250701487973602,%22roll%22%3A0%7D,%22sspParams%22%3A%7B%22type%22%3A%22SDSS_TRUE_COLOR%22,%22filter%22%3A%5B%22HSC-I%22,%22HSC-R%22,%22HSC-G%22%5D,%22simpleRgb%22%3A%7B%22beta%22%3A22026.465794806718,%22a%22%3A1,%22bias%22%3A0.05,%22b0%22%3A0%7D,%22sdssTrueColor%22%3A%7B%22beta%22%3A22026.465794806718,%22a%22%3A1,%22bias%22%3A0.05,%22b0%22%3A0%7D,%22simpleColorMatrix%22%3A%7B%22colors%22%3A%5B%7B%22filterName%22%3A%22HSC-G%22,%22enabled%22%3Atrue,%22value%22%3A%5B0,0,1%5D%7D,%7B%22filterName%22%3A%22HSC-R%22,%22enabled%22%3Atrue,%22value%22%3A%5B0,1,0%5D%7D,%7B%22filterName%22%3A%22HSC-I%22,%22enabled%22%3Atrue,%22value%22%3A%5B1,0,0%5D%7D,%7B%22filterName%22%3A%22HSC-Z%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22HSC-Y%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22NB0387%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22NB0816%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22NB0921%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22NB0527%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22NB0718%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22IB0945%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D,%7B%22filterName%22%3A%22NB0973%22,%22enabled%22%3Afalse,%22value%22%3A%5B0,0,0%5D%7D%5D,%22beta%22%3A22026.465794806718,%22a%22%3A1,%22bias%22%3A0.05,%22b0%22%3A0%7D%7D,%22externalTiles%22%3A%5B%5D,%22activeReruns%22%3A%5B%22pdr1_wide%22%5D%7D) and you can find a reference in the litterature for mapping this region of the survey in [Oguri et al. 2017](https://arxiv.org/abs/1705.06792) or more recently using an ML method in [Shirasaki et al. 2019](https://arxiv.org/abs/1911.12890). Here is the map for this region from Oguri et al:\n",
        "\n",
        "![](https://media.arxiv-vanity.com/render-output/5136459/x3.png)\n",
        "\n",
        "\n",
        "The suggested approach for solving this challenge will be based on [Remy et al. (2020)](https://arxiv.org/abs/2011.08271), Remy et al. (in prep.). This will draw on **Denoising Score Matching** approaches to learning prior distributions (see [Song and Ermon (2019)](https://arxiv.org/abs/1907.05600)).\n",
        "\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "This challenge will be an opportunity to put into practice a number of tools and methodologies. If you follow the recommended approach, you will put into practice:\n",
        "- Use Jax to write a differentiable model for weak gravitational lensing\n",
        "- Use an analytic Gaussian prior to solve the inverse problem (Wiener Filtering)\n",
        "- Use Denoising Score Matching to learn the score of a prior distribution\n",
        "- Use Stochastic Differential Equations for sampling from the posterior\n",
        "\n",
        "But you are also free to explore alternative approach if you so choose, as long as you are able to provide at the end of the day posterior samples from the mass-mapping problem.\n",
        "\n",
        "\n",
        "### Backgroung on Weak Gravitational Lensing\n",
        "\n",
        "We only provide here the fundamentals, and we encourage the interested reader to directly reach out to challenge organizers to learn more :-)\n",
        "\n",
        "Our goal is to recover an estimate of a **convergence map**, noted $\\kappa$, wich corresponds to a projected matter distribution on the sky. We can infer $\\kappa$ from the slight graviational lensing effect that affects the image of background galaxies: the apparent ellipticity of galaxies will be slightly *sheared* by a gravitational **shear** $\\gamma$. In the limit of weak lensing, the following relation holds:\n",
        "\n",
        "$$\\gamma = \\mathbf{P} \\kappa$$\n",
        "\n",
        "where $\\mathbf{P}$ is a linear lensing operator turning a convergence map $\\kappa$ into a shear map $\\gamma$.\n",
        "\n",
        "The actual measurements are the galaxy ellipticities, which have a contribution from the actual galaxy shape $e_{int}$ and a contribution from gravitational shear $\\gamma$:\n",
        "$$e_{obs} = e_{int} + \\gamma$$\n",
        "We usual assume $e_{int}$ to be Gaussian distributed, so that the full inverse problem to solve can be stated in the following form:\n",
        "\n",
        "$$e_{obs} = \\mathbf{P} \\kappa + e_{int} \\qquad \\mbox{with} \\qquad e_{int} \\sim \\mathcal{N}(0, \\sigma_e^2)$$\n",
        "\n",
        "Our goal in this challenge will be to sample the posterior $p(\\kappa | e_{obs})$.\n",
        "\n",
        "E and B modes: One last complication is that the convergence field can actually have E and B modes. Lensing to first order should only produce E modes, B modes should be 0. This notion of E and B modes will show up later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhXlunOsvT7u"
      },
      "source": [
        "## Step I: Data Preparation\n",
        "\n",
        "The shape catalog we are using can be retrieved from the HSC data release website as described [here](https://hsc-release.mtk.nao.ac.jp/doc/index.php/s16a-shape-catalog-pdr2/). For convenience we have already run the query specifically for a portion of the XMM field, and the resulting catalog is stored in the cloud. Here is the exact query:\n",
        "```sql\n",
        "select\n",
        " b.*, c.ira, c.idec, a.ishape_hsm_regauss_e1, a.ishape_hsm_regauss_e2, d.*\n",
        "from\n",
        " s16a_wide.meas2 a\n",
        " inner join s16a_wide.weaklensing_hsm_regauss b using (object_id)\n",
        " inner join s16a_wide.meas c using (object_id)\n",
        " inner join s16a_wide.photoz_mlz d using (object_id)\n",
        " where s16a_wide.search_xmm(c.skymap_id)\n",
        " AND c.ira > 34\n",
        "```\n",
        "\n",
        "\n",
        "In the rest of this section, we will be doing some data preparation, some points are slightly technical but don't worry about understanding everything as this will not matter for the rest of the challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies and accessing data"
      ],
      "metadata": {
        "id": "0ktLeF7bjnbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/EiffL/Quarks2CosmosDataChallenge.git\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!mkdir galsim\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ],
      "metadata": {
        "id": "p_56Uqv6h0QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticating and mounting cloud data storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcsfuse --implicit-dirs galsim galsim"
      ],
      "metadata": {
        "id": "rypc1fA8iK9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnf2OlpcvT7u"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "import jax\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from astropy.table import Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1xeOjX0vT7v"
      },
      "source": [
        "### Loading shape catalog\n",
        "\n",
        "For the purpose of this challenge, we will only use a 3x3 deg region of the XMM field (to keep things computationnally easy). Our first step here is to load the catalog and only keep a given region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLogzVhTvT7v"
      },
      "outputs": [],
      "source": [
        "cat = Table.read('galsim/hsc_catalog_xmm.fits')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsqXNKr1vT7w"
      },
      "outputs": [],
      "source": [
        "survey_cut = (cat['ira']>35.5) * (cat['ira']<38.5) * (cat['idec']>-6) * (cat['idec']<-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAx8oNRGvT7w"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[15,5])\n",
        "subplot(121)\n",
        "title('Input catalog')\n",
        "hist2d(cat['ira'], cat['idec'], 256);\n",
        "hlines(y=-3,xmin=35.5, xmax=38.5, color='w', linestyle='--')\n",
        "hlines(y=-6,xmin=35.5, xmax=38.5, color='w', linestyle='--')\n",
        "vlines(x=35.5,ymin=-6, ymax=-3, color='w', linestyle='--')\n",
        "vlines(x=38.5,ymin=-6, ymax=-3, color='w', linestyle='--')\n",
        "\n",
        "subplot(122)\n",
        "hist2d(cat['ira'][survey_cut], cat['idec'][survey_cut], 256);\n",
        "title('Selected catalog')\n",
        "cat = cat[survey_cut]\n",
        "suptitle('Galaxy density maps');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lxOujfqvT7w"
      },
      "source": [
        "The plot above shows us where galaxies in our sample are distributed on the sky, the holes correpond to either regions that have not been fully surveyed yet, or that are masked because of the presence of a bright star."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaI3WKVGvT7x"
      },
      "source": [
        "Another important aspect of the sample, from the analysis point of view, is to determine the redshift distribution of our galaxies, which determines how much lensing we should expect. We will just use a rough estimate here using the MLZ redshifts included in the catalog, and mostly only care about the mean redshift (**Note**: this is an overly simpltistic redshift determination, do not use directly for a science paper.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDagUeO5vT7x"
      },
      "outputs": [],
      "source": [
        "hist(cat['photoz_best'], 64);\n",
        "xlabel('z')\n",
        "ylabel('N(z)')\n",
        "axvline(mean(cat['photoz_best']),color='red')\n",
        "print('Mean redshift', np.mean(cat['photoz_best']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFZovp4LvT7x"
      },
      "source": [
        "This plot tells us that the mean redshift of our sample is z=1. This will be important for later ;-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEYy2li6vT7x"
      },
      "source": [
        "### Applying shear calibration\n",
        "\n",
        "In the introduction we oversimplified the relationship between observed ellipticity and shear. In practice it is necessary to calibrate this relation. This is for instance described in [Oguri et al. 2017](https://arxiv.org/abs/1705.06792)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfFXkeYbvT7x"
      },
      "outputs": [],
      "source": [
        "# We need to convert ellipticity to shear and calibrate it\n",
        "# Step I: computing the shear responsivity\n",
        "R = 1 - np.sum( cat['ishape_hsm_regauss_derived_shape_weight'] *\n",
        "                cat['ishape_hsm_regauss_derived_rms_e']**2) / np.sum(cat['ishape_hsm_regauss_derived_shape_weight'])\n",
        "\n",
        "# Step II: computing mean multiplicative bias factor\n",
        "mbar = np.sum( cat['ishape_hsm_regauss_derived_shape_weight'] *\n",
        "               cat['ishape_hsm_regauss_derived_shear_bias_m']) / np.sum(cat['ishape_hsm_regauss_derived_shape_weight'])\n",
        "\n",
        "cat['gamma1'] = 1./(1+mbar)*(cat['ishape_hsm_regauss_e1']/(2 * R) -\n",
        "                             cat['ishape_hsm_regauss_derived_shear_bias_c1'])\n",
        "\n",
        "cat['gamma2'] = 1./(1+mbar)*(cat['ishape_hsm_regauss_e2']/(2 * R) -\n",
        "                             cat['ishape_hsm_regauss_derived_shear_bias_c2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKh-SBi5vT7x"
      },
      "source": [
        "The important point of what happens here is that these new columns that we will call $\\gamma_{obs}$, derived from measured ellipticities `ishape_hsm_regauss_e1/2` should now follow the relation:\n",
        "$$\\gamma_{obs} = \\gamma + n$$\n",
        "where $n$ is again assumed to be a Gaussian noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1vQaAMKvT7x"
      },
      "source": [
        "### Creating maps of binned ellipticity\n",
        "\n",
        "Our next step is to create maps by binning the ellipticities of galaxies on a grid using the [lenspack](https://github.com/CosmoStat/lenspack/tree/master/lenspack) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LqFAiZLvT7x"
      },
      "outputs": [],
      "source": [
        "from lenspack.utils import bin2d\n",
        "from quarks2cosmos.utils import get_extend_radec\n",
        "\n",
        "# Compute boundaries of the field so that it fits in a 360x360 map\n",
        "# MassiveNuS maps beging 512x512 with 0.4 arcmin resolution, we resize them to 360x360\n",
        "resolution = 512/360*0.4                     # Pixel resolution in arcmin/pixel\n",
        "pixel_size = np.pi * resolution / 180. / 60. # Pixel resolution in rad/pixel\n",
        "\n",
        "width = 360                                  # Number of pixels in output map\n",
        "size = width * resolution / 60.              # Angular size of output map in deg.\n",
        "\n",
        "# Compute center of the catalog\n",
        "ra = median(cat['ira'])\n",
        "dec = median(cat['idec'])\n",
        "\n",
        "# Determine extent of our map\n",
        "extent = [ra - size/2, ra + size/2, dec - size/2, dec + size/2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXgGBfj2vT7y"
      },
      "source": [
        "**Number of galaxy per pixel map**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dZ1BhG-vT7y"
      },
      "outputs": [],
      "source": [
        "# Make a map of number of galaxies per pixels\n",
        "n_gal_map = bin2d(cat['ira'], cat['idec'], npix=width, extent=extent)\n",
        "imshow(n_gal_map, origin='lower', extent=get_extend_radec(resolution, width,ra, dec)); colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlFbCp-jvT7y"
      },
      "source": [
        "**Survey mask**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hy-mG6svT7y"
      },
      "outputs": [],
      "source": [
        "# Compute survey mask indicating regions where no galaxies are observed\n",
        "m = n_gal_map == 0\n",
        "mask = 1. - m\n",
        "imshow(mask, cmap='magma', origin='lower', extent=get_extend_radec(resolution, width, ra, dec))\n",
        "colorbar()\n",
        "title(\"binary mask\")\n",
        "mask = np.expand_dims(mask, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVfIkPvsvT7y"
      },
      "source": [
        "**Ellipticity maps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSslaLdxvT7y"
      },
      "outputs": [],
      "source": [
        "# Bin ellipticity components based on galaxy position into a 360x360 map\n",
        "g1map, g2map = bin2d(cat['ira'], cat['idec'],\n",
        "                     v=(cat['gamma1'], cat['gamma2']),\n",
        "                     w=cat['ishape_hsm_regauss_derived_shape_weight'],\n",
        "                     npix=width,\n",
        "                     extent=extent)\n",
        "\n",
        "figure(figsize=[10,5])\n",
        "subplot(121)\n",
        "imshow(g1map, cmap='magma', origin='lower', extent=get_extend_radec(resolution, width, ra, dec),vmin=-0.3,vmax=0.3)\n",
        "colorbar()\n",
        "xlabel('ra')\n",
        "ylabel('dec')\n",
        "title('g1')\n",
        "\n",
        "subplot(122)\n",
        "imshow(g2map, cmap='magma', origin='lower', extent=get_extend_radec(resolution, width, ra, dec),vmin=-0.3,vmax=0.3)\n",
        "colorbar()\n",
        "xlabel('ra')\n",
        "title('g2')\n",
        "\n",
        "# This will act as our input data\n",
        "g_obs = np.stack([g1map, g2map], axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc7nyw0qvT7y"
      },
      "source": [
        "**Noise covariance matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3gr9Vq3vT7y"
      },
      "outputs": [],
      "source": [
        "m = n_gal_map == 0\n",
        "sigma_g1 = np.std(cat['gamma1'])/np.sqrt(n_gal_map)\n",
        "sigma_g1[m] = 1e3 # Setting unobserved pixels to high noise\n",
        "sigma_g2 = np.std(cat['gamma2'])/np.sqrt(n_gal_map)\n",
        "sigma_g2[m] = 1e3 # Setting unobserved pixels to high noise\n",
        "\n",
        "figure(figsize=[10, 5])\n",
        "subplot(121)\n",
        "imshow(sigma_g1, extent=get_extend_radec(resolution, width, ra, dec),vmax=0.3)\n",
        "title(r'$\\sigma_{e_1}/\\sqrt{N_{gal}}$')\n",
        "colorbar()\n",
        "\n",
        "subplot(122)\n",
        "imshow(sigma_g2, extent=get_extend_radec(resolution, width, ra, dec),vmax=0.3)\n",
        "title(r'$\\sigma_{e_2}/\\sqrt{N_{gal}}$')\n",
        "colorbar()\n",
        "\n",
        "# This will be our estimate of the noise covariance in the shear map\n",
        "g_obs_std = jnp.stack([sigma_g1, sigma_g2],axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMOnFCtgvT7y"
      },
      "source": [
        "What we see in these maps of the standard deviation in the observed g1, g2 is that because of masked regions and varying number of galaxies per pixels, the expected noise can differ significantly from pixel to pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVqmZJ4vT7y"
      },
      "source": [
        "### Summary of available data\n",
        "\n",
        "At this point, our data is all prepared. In the next steps of the notebook, we will use these two arrays:\n",
        "- `g_obs`: [360,360,2] contains the two components of the measured shear\n",
        "- `g_obs_std`: [360,360,2] contains the standard deviation of noise (coming from intrinsic galaxy ellipticity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCtKM6NhvT7y"
      },
      "source": [
        "## Step II: Traditional Mass-Mapping using the Kaiser-Squires estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAM0BpfwvT7y"
      },
      "source": [
        "Before diving into anything more complicated, we will use the standard reconstruction technique, proposed in\n",
        "[Kaiser-Squires (1993)](https://ui.adsabs.harvard.edu/abs/1993ApJ...404..441K/abstract).\n",
        "\n",
        "This consists in a \"direct inversion\" by inverting the lensing operator $\\mathbf{P}$:\n",
        "$$ \\tilde{\\kappa} = \\mathbf{P}^{-1} (\\gamma + n) $$\n",
        "\n",
        "This solution would actually work in the limit of no missing data. Contrary to operators like a PSF convolution, inverting this lensing operator $\\mathbf{P}$ preserves the noise properties (no noise amplification) and in the absence of noise would perfectly recover the convergence.\n",
        "\n",
        "In practice however, noise and missing data (masked areas) corrupt the output map. It is conventional to apply a Gaussian smoothing to at least reduce the noise variance in the reconstructed map.\n",
        "\n",
        "\n",
        "We will use a concrete implementation of the $\\mathbf{P}$ operator provided as part of the `quarks2cosmos` package, using FFTs under the hood, and written in Jax (this will be usefull for later ;-) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Hdod8NvT7z"
      },
      "outputs": [],
      "source": [
        "from quarks2cosmos.lensing import ks93\n",
        "from scipy.ndimage import gaussian_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP8nH2XrvT7z"
      },
      "outputs": [],
      "source": [
        "kappaE, kappaB = ks93(g1map, g2map) # This computes P^{1} \\gamma\n",
        "\n",
        "smoothing = 2 / resolution # smoothing in pixel units corresponding to 2 arcmin\n",
        "\n",
        "kwargs = dict(vmin=-.05, vmax=.1, cmap='magma', extent=get_extend_radec(resolution, width, ra, dec), origin='lower')\n",
        "figure(figsize=[12.5,5])\n",
        "subplot(121)\n",
        "imshow(gaussian_filter(kappaE,smoothing), **kwargs);colorbar()\n",
        "title('E-mode')\n",
        "subplot(122)\n",
        "imshow(gaussian_filter(kappaB,smoothing), **kwargs);colorbar()\n",
        "title('B-mode')\n",
        "\n",
        "suptitle('Kaiser-Squires reconstruction (2 arcmin smoothing)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz6gOHS4vT7z"
      },
      "source": [
        "Congratulations! If you have never done that before, you have just made your first dark matter map from weak gravitational lensing!\n",
        "\n",
        "\n",
        "What we see here is that we see some high intensity peaks in the E-mode (which is where we expect physical lensing signal) and no discernable features in the B-mode map (which should only contain noise)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XaLOZglvT7z"
      },
      "source": [
        "## Step III: Let's start to get creative! Writing a Wiener filter in Jax\n",
        "\n",
        "\n",
        "The Kaiser-Squires inversion is a maximum likelihood approach, which doesn't assume a prior on the signal, it therefore cannot correct in any way the impact of noise and missing data.\n",
        "\n",
        "\n",
        "We can now try to solve the inverse problem by optimization, and we can start by assuming an analytic Gaussian prior on the convergence field, this should allow us to compute easily a Maximum a Posterior (MAP) solution.\n",
        "\n",
        "The idea here is that we will want to compute the maximum of the following distribution:\n",
        "\n",
        "$$p(\\kappa | \\gamma_{obs}) \\  \\propto \\  p(\\gamma_{obs} | \\kappa) \\ p_{gaussian}(\\kappa)  $$\n",
        "\n",
        "More precisely, we will compute this MAP solution (also known as the Wiener filter solution in the case of a Gaussian prior) as:\n",
        "\n",
        "$$ \\kappa_{W} = \\arg\\max_{\\kappa} \\log p(\\gamma_{obs} | \\kappa) + \\log p_{gaussian}(\\kappa)  $$\n",
        "\n",
        "\n",
        "### Challenge objectives\n",
        "\n",
        "You can now start to apply what you will have seen in the guided tutorial. In this section, you will need to build the following:\n",
        "\n",
        "- [x] : Build a Gaussian prior function in Jax (I'm helping for this one ;-) you are welcome)\n",
        "- [ ] : Build a likelihood function in Jax and TensorFlow Probability\n",
        "- [ ] : Write an optimization loop to obtain the Wiener solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OXHo6CBvT7z"
      },
      "source": [
        "#### Build theoretical Gaussian prior\n",
        "\n",
        "Here we will use Cosmology theory to predict the expected power spectrum of the convergence field, for a given set of cosmological parameters and redshift distribution. We'll use the handy [jax-cosmo](https://github.com/DifferentiableUniverseInitiative/jax_cosmo) library for that.\n",
        "\n",
        "If you are not a cosmologist, you may safely go quicly over this section. Ask challenge organizers if you have any questions ;-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4srjvd6vT7z"
      },
      "outputs": [],
      "source": [
        "import jax_cosmo as jc\n",
        "\n",
        "# Create model of the redshift distribution\n",
        "pz = jc.redshift.kde_nz(jnp.array(cat['photoz_best'][:10000].astype('float32')),\n",
        "                        jnp.ones(10000), bw=0.05)\n",
        "z = jnp.linspace(0,3,256)\n",
        "\n",
        "hist(cat['photoz_best'], 32,range=[0,3],density=True, label='MLZ z-best');\n",
        "plot(z, pz(z), label='model p(z)');\n",
        "xlabel('z');\n",
        "legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWRncnlivT7z"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def theory_cl(ell):\n",
        "    \"\"\" Computes angular C_ell for our survey, at given ell\n",
        "    \"\"\"\n",
        "    cosmo = jc.Planck15()\n",
        "    tracer = jc.probes.WeakLensing([pz])\n",
        "    return jc.angular_cl.angular_cl(cosmo, ell, [tracer])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVAyAXGGvT7z"
      },
      "outputs": [],
      "source": [
        "ell = jnp.logspace(2,5)\n",
        "loglog(ell, theory_cl(ell), label='Theory');\n",
        "xlabel('$\\ell$')\n",
        "ylabel('$C_\\ell$')\n",
        "title('Lensing angular power spectrum');\n",
        "legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EluO2WrxvT7z"
      },
      "source": [
        "Given this theory power spectrum, we now just need to write a function that will compute the log prior value for a given map:\n",
        "\n",
        "$$ \\log p_{Gaussian}(\\kappa) = - \\frac{1}{2} \\frac{\\hat{\\kappa}^*\\hat{\\kappa}}{ \\mathbf{S}} + cst$$\n",
        "\n",
        "where $\\hat{\\kappa}$ is the Fourier transform of the convergence field, annd $\\mathbf{S}$ is the theory power spectrum shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er3wRQnZvT7z"
      },
      "outputs": [],
      "source": [
        "from quarks2cosmos.lensing import make_power_map\n",
        "\n",
        "# Computing theory power spectrum\n",
        "ell = jnp.logspace(0,5,256)\n",
        "ps = theory_cl(ell)\n",
        "ps_data = jnp.stack([ell, ps])\n",
        "\n",
        "# Build diagonal covariance matrix S\n",
        "S = make_power_map(ps_data, size=360, pixel_size=pixel_size, ell=True)\n",
        "\n",
        "def log_gaussian_prior(kappa):\n",
        "    \"\"\" Computes the log gaussian prior using theoretical power spectrum\n",
        "        Args:\n",
        "            kappa: [360,360,2] array of E and B components of convergence\n",
        "        Returns:\n",
        "            Value of log prior\n",
        "    \"\"\"\n",
        "    kE_ft = jnp.fft.fft2(kappa[...,0]) / 360. # Normalisation by size of map\n",
        "    kB_ft = jnp.fft.fft2(kappa[...,1]) / 360.\n",
        "    k_ft = jnp.stack([kE_ft, kB_ft],axis=-1)\n",
        "\n",
        "    # For the E mode, we use the S matrix as the prior\n",
        "    # For the B mode, since they should be 0 from theory, we have a very low amplitude prior\n",
        "    variance = jnp.stack([S,                      # E mode prior\n",
        "                          jnp.ones_like(S)*1e-9], # B mode prior\n",
        "                          axis=-1)\n",
        "\n",
        "    return -0.5*jnp.sum(jnp.abs(k_ft*jnp.conj(k_ft)) / (variance))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnD1EflEvT7z"
      },
      "outputs": [],
      "source": [
        "# We can now for instance evaluate the log prior of the Kaiser-Squires solution\n",
        "k_KS = jnp.stack([kappaE, kappaB],axis=-1)\n",
        "\n",
        "log_gaussian_prior(k_KS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdlj34IvvT73"
      },
      "source": [
        "Ok, so the prior is very low for the KS solution.... probably because the prior really doesn't like the non-zero B mode\n",
        "\n",
        "**Summary** :  We now have a `log_gaussian_prior` function that returns $\\log p_{Gaussian}(\\kappa)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC1X-UlqvT73"
      },
      "source": [
        "#### Build data likelihood\n",
        "\n",
        "This section is now up to you, remember that you have the following elements at your disposal:\n",
        "- `g_obs`: [360,360,2] contains the two components of the measured shear\n",
        "- `g_obs_std`: [360,360,2] contains the standard deviation of noise (coming from intrinsic galaxy ellipticity)\n",
        "\n",
        "We need to complete the function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4s4VFnpvT73"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHItXn8qvT74"
      },
      "outputs": [],
      "source": [
        "def log_likelihood(kappa):\n",
        "    \"\"\" Computes the log likelihood of observed shear for a given convergence map\n",
        "        Args:\n",
        "            kappa: [360,360,2] array of E and B components of convergence\n",
        "        Returns:\n",
        "            Value of log likelihood p(g_obs | \\kappa)\n",
        "    \"\"\"\n",
        "    # Build the forward model that goes form convergence to shear\n",
        "\n",
        "    g = xxxx\n",
        "\n",
        "    # Write down the likelihood distribution of the data\n",
        "    likelihood = tfd.xxx#....\n",
        "\n",
        "    # What should be in this xxxxx ?\n",
        "    return likelihood.log_prob(xxxxx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N6BvPh9vT74"
      },
      "outputs": [],
      "source": [
        "# If everything works, you should be able to run this:\n",
        "log_likelihood(k_KS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_05n1_0vT74"
      },
      "source": [
        "#### Write an optimizer to retrieve MAP solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGFTtP1rvT74"
      },
      "outputs": [],
      "source": [
        "# We will need a function that return the log posterior\n",
        "log_posterior = lambda x: xxxxxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INP27xq3vT74"
      },
      "outputs": [],
      "source": [
        "# Initialize an optimizer and a variable\n",
        "\n",
        "# Create an optimization param\n",
        "x = jnp.zeros([360, 360, 2])\n",
        "\n",
        "# ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZJmkVzdvT74"
      },
      "outputs": [],
      "source": [
        "# write an update function\n",
        "@jax.jit\n",
        "def update(x, opt_state):\n",
        "    # ....\n",
        "    return logp, x, opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIsBnf2avT74"
      },
      "outputs": [],
      "source": [
        "# Run the optimization\n",
        "losses = []\n",
        "for i in range(200):\n",
        "    logp, x, opt_state = update(x, opt_state)\n",
        "    losses.append(logp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2c6OgdNvT74"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss function\n",
        "loglog(abs(np.array(losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rougD3uNvT74"
      },
      "outputs": [],
      "source": [
        "# Here is the result\n",
        "figure(figsize=[15,5])\n",
        "subplot(131)\n",
        "imshow(gaussian_filter(kappaE,smoothing),**kwargs)\n",
        "title('Kaiser-Squires E-mode')\n",
        "subplot(132)\n",
        "imshow(x[...,0],**kwargs);\n",
        "title('Wiener E-mode')\n",
        "subplot(133)\n",
        "imshow(x[...,1],**kwargs);\n",
        "title('Wiener B-mode');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CcmJJ1XvT74"
      },
      "outputs": [],
      "source": [
        "from quarks2cosmos.lensing import measure_power_spectrum\n",
        "\n",
        "# We can also compute the power spectra of these various maps\n",
        "ell, ps_ks = measure_power_spectrum(kappaE,\n",
        "                                      pixel_size=pixel_size)\n",
        "\n",
        "ell, ps_ks_smooth = measure_power_spectrum(gaussian_filter(kappaE,smoothing),\n",
        "                                      pixel_size=pixel_size)\n",
        "\n",
        "ell, ps_wiener = measure_power_spectrum(x[...,0],\n",
        "                                        pixel_size=pixel_size)\n",
        "\n",
        "cl  = theory_cl(ell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44GiBd7_vT74"
      },
      "outputs": [],
      "source": [
        "loglog(ell, cl,color='gray',label='Theory')\n",
        "loglog(ell, ps_ks, color='C1', label='Kaiser-Squires')\n",
        "loglog(ell, ps_ks_smooth, '--', color='C1', label='Kaiser-Squires Smoothed')\n",
        "\n",
        "loglog(ell, ps_wiener,color='red', label='Wiener')\n",
        "legend()\n",
        "xlim(2e2,1e4)\n",
        "ylim(1e-13,1e-7)\n",
        "xlabel('Angular Multipole ${\\ell}$');\n",
        "ylabel('$C_{\\ell}$');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIl4WB_vvT74"
      },
      "source": [
        "All right :-) So did you retrieve a MAP solution? If so, don't hesitate to share with the group!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqLdf162vT74"
      },
      "source": [
        "## Taking the training wheels off: Can you propose a solution with Machine Learning?\n",
        "\n",
        "Thanks for bearing with the introduction of this notebook! You now should have all you need to take on this challenge by yourself, using the tools above and the methodologies that will be presented in the guided challenge.\n",
        "\n",
        "\n",
        "**The goal**: sample from the full posterior:\n",
        "\n",
        "$$ p(\\kappa | \\gamma_{obs}) \\propto p(\\gamma_{obs} | \\kappa) p(\\kappa) $$\n",
        "\n",
        "And instead of using an analytic Gaussian prior, learn this prior from simulations.\n",
        "\n",
        "\n",
        "To provide some guidance, here would be a good set of steps to follow:\n",
        "\n",
        "- Learn a score model using Denoising Score Matching\n",
        "- Use an SDE-based sampler to sample solutions from the posterior using only the score\n",
        "\n",
        "We will see these notions in the Guided challenge. But you can also try any other approach, for instance a VAE... but good luck with that :-P\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Load the MassiveNuS convergence dataset\n",
        "\n",
        "The only help you will get in this section is how to access the set of simulations we can use to learn the prior. We are using the MassiveNuS dataset [(Liu et al 2017)](https://arxiv.org/abs/1711.10524), available at  http://www.columbialensing.org/#massivenus.\n",
        "\n",
        "These simulations contain simulated convergence maps of 512x512 pixels, representing an area of 12.25 deg^2, i.e. with 0.4 arcmin resolution. The convergence maps are available for a few source redshifts. For the purpose of this challenge, we have selected the simulations at the fiducial cosmology, and only used the source plane $z_s=1$ as it corresponds to the mean redshift of our lensing sample.\n",
        "\n",
        "**Note**: There are several caveats with this setting, first of all using a single source plane at the mean redshift is not good enough, second MassiveNuS have limited resolution and shouldn't be trusted down to the arcmin resolution. If you are interested in doing a full scientifically exploitable map, have a look at the simulations used in [Shirasaki et al. 2019](https://arxiv.org/abs/1911.12890)\n",
        "\n",
        "\n",
        "Below you will find the code needed to load these maps and resize them to the pixel resolution we use in this notebook (leading to 360x360 maps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KouOTTGvvT75"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import quarks2cosmos.datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEGvc-sKvT75"
      },
      "outputs": [],
      "source": [
        "def map_fn(example):\n",
        "    # Apply data augmentation\n",
        "    k = tf.expand_dims(example['map'], -1)\n",
        "    k = tf.image.flip_left_right(k)\n",
        "    k = tf.image.flip_up_down(k)\n",
        "\n",
        "    # Apply image reampling to 360x360 but preservin\n",
        "    k = tf.image.resize(k, [360, 360],\n",
        "                        method=tf.image.ResizeMethod.AREA)\n",
        "\n",
        "    # Removing batch dimension\n",
        "    return k[...,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nX_xx6gvT75"
      },
      "outputs": [],
      "source": [
        "massive_nu = tfds.load('MassiveNu', split=tfds.Split.TRAIN,\n",
        "                       data_dir='galsim/tensorflow_datasets')\n",
        "massive_nu = massive_nu.map(map_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3NIoUYLvT75"
      },
      "outputs": [],
      "source": [
        "dset = massive_nu.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibpr3FaOvT75"
      },
      "outputs": [],
      "source": [
        "m = next(dset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki9pVr4EvT75"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[10,10])\n",
        "imshow(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRUm04xOvT75"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}